{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# LLM Response Analysis: Cancer Survival Prediction (Improved)\n",
                "\n",
                "This notebook analyzes the LLM's predictions with an **improved prediction extraction** that:\n",
                "1. Handles negation (e.g., 'not survive' -> 0)\n",
                "2. Detects conflicting information (e.g., says '1' but text mentions 'death')\n",
                "\n",
                "**Metrics computed:**\n",
                "- Confusion Matrix\n",
                "- Accuracy, Precision, Recall, F1-Score\n",
                "- Classification Report by cancer system"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "import re\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.metrics import (\n",
                "    confusion_matrix, \n",
                "    classification_report, \n",
                "    accuracy_score,\n",
                "    precision_score, \n",
                "    recall_score, \n",
                "    f1_score,\n",
                "    ConfusionMatrixDisplay\n",
                ")\n",
                "\n",
                "# Set plot style\n",
                "plt.style.use('seaborn-v0_8-whitegrid')\n",
                "sns.set_palette('husl')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Load Results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load the results file\n",
                "RESULTS_FILE = 'results_rag.json'  # Update path if needed\n",
                "\n",
                "with open(RESULTS_FILE, 'r') as f:\n",
                "    results = json.load(f)\n",
                "\n",
                "print(f\"Loaded {len(results)} results\")\n",
                "print(f\"\\nSample result keys: {list(results[0].keys())}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Improved Prediction Extraction\n",
                "\n",
                "Key improvements:\n",
                "- **Conflict Detection**: Tracks both survival and death signals separately\n",
                "- **Negation Handling**: 'not survive' -> 0, 'not die' -> 1\n",
                "- **End-of-text Priority**: Trusts explicit 0/1 at the end of response as tie-breaker"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def extract_prediction(response: str) -> int | None:\n",
                "    \"\"\"\n",
                "    Extract the binary prediction (0 or 1) from the LLM response.\n",
                "    Returns None if:\n",
                "    1. No prediction is found\n",
                "    2. Conflicting information is detected (e.g., explicit '1' but text says 'death')\n",
                "    \"\"\"\n",
                "    response = response.strip().lower()\n",
                "    \n",
                "    # Track detected signals\n",
                "    detected_1 = False\n",
                "    detected_0 = False\n",
                "    \n",
                "    # 1. Check for Explicit Match at End (Strongest Signal)\n",
                "    end_match = re.search(r'\\b([01])\\s*[.]?(\\s*<\\|assistant_end\\|>)?$', response)\n",
                "    if end_match:\n",
                "        val = int(end_match.group(1))\n",
                "        if val == 1: detected_1 = True\n",
                "        else: detected_0 = True\n",
                "    \n",
                "    # 2. Check for 'Prediction: X' patterns\n",
                "    patterns = [\n",
                "        r'(?:prediction|answer|result|outcome|value)[\\s:]+([01])\\b',\n",
                "        r'\\breturn\\s+([01])\\b',\n",
                "    ]\n",
                "    for pattern in patterns:\n",
                "        match = re.search(pattern, response)\n",
                "        if match:\n",
                "            val = int(match.group(1))\n",
                "            if val == 1: detected_1 = True\n",
                "            else: detected_0 = True\n",
                "\n",
                "    # 3. Check for Textual Keywords\n",
                "    \n",
                "    # Negation Handling (CRITICAL)\n",
                "    if re.search(r'\\b(not|unlikely to|fail to|did not) (survive|live|recover)\\b', response):\n",
                "        detected_0 = True\n",
                "    if re.search(r'\\b(not|unlikely to) (die|succumb)\\b', response):\n",
                "        detected_1 = True\n",
                "        \n",
                "    # Standard Keywords\n",
                "    if re.search(r'\\b(survive|survival|lives|alive)\\b', response):\n",
                "        detected_1 = True\n",
                "    if re.search(r'\\b(die|death|dead|deceased|succumb)\\b', response):\n",
                "        detected_0 = True\n",
                "    \n",
                "    # 4. Resolve Conflict\n",
                "    if detected_1 and detected_0:\n",
                "        # CONFLICT! Trust end-of-text match if present\n",
                "        if end_match:\n",
                "            return int(end_match.group(1))\n",
                "        return None  # Genuinely contradictory\n",
                "        \n",
                "    if detected_1: return 1\n",
                "    if detected_0: return 0\n",
                "    \n",
                "    return None\n",
                "\n",
                "# Test the extraction function\n",
                "test_responses = [\n",
                "    \"1\",\n",
                "    \"0\",\n",
                "    \"The patient will survive.\",\n",
                "    \"The patient will not survive.\",\n",
                "    \"Based on the data, I predict 0 (death).\",\n",
                "    \"The patient is predicted to die.0<|assistant_end|>\",  # Conflict resolved by end\n",
                "    \"The patient will survive but may die soon.\",  # Conflict, no end -> None\n",
                "]\n",
                "\n",
                "print(\"Testing extraction function:\")\n",
                "for resp in test_responses:\n",
                "    print(f\"  '{resp[:50]}...' -> {extract_prediction(resp)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Extract predictions and ground truth\n",
                "data = []\n",
                "\n",
                "for item in results:\n",
                "    prediction = extract_prediction(item['response'])\n",
                "    \n",
                "    # Ground truth: died_from_cancer (1 = died from cancer, 0 = alive or died from other cause)\n",
                "    # Note: We're predicting SURVIVAL (1 = survive, 0 = death)\n",
                "    # So we need to invert: if died_from_cancer=1, then survival=0\n",
                "    died_from_cancer = int(item['ground_truth']['died_from_cancer'])\n",
                "    ground_truth_survival = 1 - died_from_cancer  # Invert for survival prediction\n",
                "    \n",
                "    data.append({\n",
                "        'patient_id': item.get('patient_id'),\n",
                "        'response': item['response'],\n",
                "        'prediction': prediction,\n",
                "        'ground_truth_survival': ground_truth_survival,\n",
                "        'died_from_cancer': died_from_cancer,\n",
                "        'is_alive': int(item['ground_truth']['is_alive']),\n",
                "        'cancer_system': item.get('features', {}).get('cancer_system', 'Unknown'),\n",
                "        'survival_months': float(item['ground_truth'].get('survival_months', 0)),\n",
                "    })\n",
                "\n",
                "df = pd.DataFrame(data)\n",
                "print(f\"Total samples: {len(df)}\")\n",
                "print(f\"Samples with valid predictions: {df['prediction'].notna().sum()}\")\n",
                "print(f\"Samples with missing/conflicting predictions: {df['prediction'].isna().sum()}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Show samples where prediction extraction failed (likely conflicts)\n",
                "failed_extractions = df[df['prediction'].isna()]\n",
                "if len(failed_extractions) > 0:\n",
                "    print(f\"\\nFailed/Conflicting extractions ({len(failed_extractions)}):\")\n",
                "    for idx, row in failed_extractions.head(5).iterrows():\n",
                "        print(f\"\\n  Response: {row['response'][:200]}...\")\n",
                "else:\n",
                "    print(\"All predictions extracted successfully!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Filter to valid predictions only\n",
                "df_valid = df[df['prediction'].notna()].copy()\n",
                "df_valid['prediction'] = df_valid['prediction'].astype(int)\n",
                "\n",
                "print(f\"\\nValid predictions: {len(df_valid)} / {len(df)} ({100*len(df_valid)/len(df):.1f}%)\")\n",
                "print(f\"\\nPrediction distribution:\")\n",
                "print(df_valid['prediction'].value_counts())\n",
                "print(f\"\\nGround truth distribution (survival):\")\n",
                "print(df_valid['ground_truth_survival'].value_counts())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Confusion Matrix"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "y_true = df_valid['ground_truth_survival'].values\n",
                "y_pred = df_valid['prediction'].values\n",
                "\n",
                "# Compute confusion matrix\n",
                "cm = confusion_matrix(y_true, y_pred)\n",
                "\n",
                "# Plot confusion matrix\n",
                "fig, ax = plt.subplots(figsize=(8, 6))\n",
                "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Death (0)', 'Survival (1)'])\n",
                "disp.plot(cmap='Blues', ax=ax, values_format='d')\n",
                "plt.title('Confusion Matrix: LLM Survival Predictions (Improved)', fontsize=14)\n",
                "plt.tight_layout()\n",
                "plt.savefig('confusion_matrix_improved.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "# Print raw values\n",
                "print(\"\\nConfusion Matrix:\")\n",
                "print(f\"  TN (True Death):     {cm[0,0]}\")\n",
                "print(f\"  FP (False Survival): {cm[0,1]}\")\n",
                "print(f\"  FN (False Death):    {cm[1,0]}\")\n",
                "print(f\"  TP (True Survival):  {cm[1,1]}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Classification Metrics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calculate metrics\n",
                "accuracy = accuracy_score(y_true, y_pred)\n",
                "precision = precision_score(y_true, y_pred, zero_division=0)\n",
                "recall = recall_score(y_true, y_pred, zero_division=0)\n",
                "f1 = f1_score(y_true, y_pred, zero_division=0)\n",
                "\n",
                "# Specificity (true negative rate)\n",
                "tn, fp, fn, tp = cm.ravel()\n",
                "specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
                "\n",
                "# Print metrics\n",
                "print(\"=\"*50)\n",
                "print(\"CLASSIFICATION METRICS (Improved Extraction)\")\n",
                "print(\"=\"*50)\n",
                "print(f\"Accuracy:    {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
                "print(f\"Precision:   {precision:.4f}\")\n",
                "print(f\"Recall:      {recall:.4f} (Sensitivity)\")\n",
                "print(f\"Specificity: {specificity:.4f}\")\n",
                "print(f\"F1-Score:    {f1:.4f}\")\n",
                "print(\"=\"*50)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Full classification report\n",
                "print(\"\\nDetailed Classification Report:\")\n",
                "print(classification_report(y_true, y_pred, target_names=['Death (0)', 'Survival (1)']))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Summary\n",
                "\n",
                "This notebook demonstrates improved prediction extraction with:\n",
                "- Negation handling\n",
                "- Conflict detection\n",
                "- End-of-text priority for tie-breaking"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}