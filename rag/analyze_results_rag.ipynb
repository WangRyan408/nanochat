{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# LLM Response Analysis: Cancer Survival Prediction\n",
                "\n",
                "This notebook analyzes the LLM's predictions and compares them to ground truth cancer outcome data.\n",
                "\n",
                "**Metrics computed:**\n",
                "- Confusion Matrix\n",
                "- Accuracy, Precision, Recall, F1-Score\n",
                "- AUROC (Area Under ROC Curve)\n",
                "- AUPRC (Area Under Precision-Recall Curve)\n",
                "- Classification Report by cancer system"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "import re\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.metrics import (\n",
                "    confusion_matrix, \n",
                "    classification_report, \n",
                "    accuracy_score,\n",
                "    precision_score, \n",
                "    recall_score, \n",
                "    f1_score,\n",
                "    roc_auc_score,\n",
                "    roc_curve,\n",
                "    precision_recall_curve,\n",
                "    average_precision_score,\n",
                "    ConfusionMatrixDisplay\n",
                ")\n",
                "\n",
                "# Set plot style\n",
                "plt.style.use('seaborn-v0_8-whitegrid')\n",
                "sns.set_palette('husl')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Load Results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loaded 100 results\n",
                        "\n",
                        "Sample result keys: ['prompt', 'response', 'patient_id', 'ground_truth', 'features']\n"
                    ]
                }
            ],
            "source": [
                "import json\n",
                "\n",
                "# Load the results file\n",
                "RESULTS_FILE = 'results_rag.json'  # Update path if needed\n",
                "\n",
                "with open(RESULTS_FILE, 'r') as f:\n",
                "    results = json.load(f)\n",
                "\n",
                "print(f\"Loaded {len(results)} results\")\n",
                "print(f\"\\nSample result keys: {list(results[0].keys())}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Display a sample result\n",
                "print(\"Sample prompt (truncated):\")\n",
                "print(results[0]['prompt'][:500], \"...\")\n",
                "print(\"\\n\" + \"=\"*50)\n",
                "print(\"\\nSample response:\")\n",
                "print(results[0]['response'])\n",
                "print(\"\\n\" + \"=\"*50)\n",
                "print(\"\\nGround truth:\")\n",
                "print(json.dumps(results[0]['ground_truth'], indent=2))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Extract Predictions from LLM Responses\n",
                "\n",
                "The LLM is asked to return `1` for survival and `0` for death. We need to parse the response to extract the prediction."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def extract_prediction(response: str) -> int | None:\n",
                "    \"\"\"\n",
                "    Extract the binary prediction (0 or 1) from the LLM response.\n",
                "    Returns None if no clear prediction is found.\n",
                "    \"\"\"\n",
                "    response = response.strip().lower()\n",
                "    \n",
                "    # Try to find explicit 0 or 1 in the response\n",
                "    # Look for patterns like \"0\", \"1\", \"prediction: 0\", \"return 1\", etc.\n",
                "    \n",
                "    # Check for explicit \"0\" or \"1\" as the only content or at the end\n",
                "    if response == '0' or response == '1':\n",
                "        return int(response)\n",
                "    \n",
                "    # Look for patterns like \"prediction: 1\" or \"answer: 0\"\n",
                "    patterns = [\n",
                "        r'(?:prediction|answer|result|outcome)[:\\s]+([01])\\b',\n",
                "        r'\\breturn[s]?\\s+(?:a\\s+)?([01])\\b',\n",
                "        r'\\b(?:predicted|predict)\\s+(?:survival\\s+)?(?:outcome)?[:\\s]*([01])\\b',\n",
                "        r'\\b([01])\\s*(?:\\.|$)',  # 0 or 1 at end of sentence or response\n",
                "    ]\n",
                "    \n",
                "    for pattern in patterns:\n",
                "        match = re.search(pattern, response)\n",
                "        if match:\n",
                "            return int(match.group(1))\n",
                "    \n",
                "    # Look for survival/death keywords if no explicit number\n",
                "    survival_keywords = ['survive', 'survival', 'alive', 'will live', 'likely to survive']\n",
                "    death_keywords = ['death', 'die', 'dead', 'will not survive', 'unlikely to survive', 'deceased']\n",
                "    \n",
                "    for keyword in survival_keywords:\n",
                "        if keyword in response:\n",
                "            return 1\n",
                "    \n",
                "    for keyword in death_keywords:\n",
                "        if keyword in response:\n",
                "            return 0\n",
                "    \n",
                "    return None  # Could not determine prediction\n",
                "\n",
                "# Test the extraction function\n",
                "test_responses = [\n",
                "    \"1\",\n",
                "    \"0\",\n",
                "    \"The predicted survival outcome is 1.\",\n",
                "    \"Based on the data, I predict 0 (death).\",\n",
                "    \"This patient is likely to survive.\",\n",
                "]\n",
                "\n",
                "print(\"Testing extraction function:\")\n",
                "for resp in test_responses:\n",
                "    print(f\"  '{resp}' -> {extract_prediction(resp)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Extract predictions and ground truth\n",
                "data = []\n",
                "\n",
                "for item in results:\n",
                "    prediction = extract_prediction(item['response'])\n",
                "    \n",
                "    # Ground truth: died_from_cancer (1 = died from cancer, 0 = alive or died from other cause)\n",
                "    # Note: We're predicting SURVIVAL (1 = survive, 0 = death)\n",
                "    # So we need to invert: if died_from_cancer=1, then survival=0\n",
                "    died_from_cancer = int(item['ground_truth']['died_from_cancer'])\n",
                "    ground_truth_survival = 1 - died_from_cancer  # Invert for survival prediction\n",
                "    \n",
                "    data.append({\n",
                "        'patient_id': item.get('patient_id'),\n",
                "        'response': item['response'],\n",
                "        'prediction': prediction,\n",
                "        'ground_truth_survival': ground_truth_survival,\n",
                "        'died_from_cancer': died_from_cancer,\n",
                "        'is_alive': int(item['ground_truth']['is_alive']),\n",
                "        'cancer_system': item.get('features', {}).get('cancer_system', 'Unknown'),\n",
                "        'survival_months': float(item['ground_truth'].get('survival_months', 0)),\n",
                "    })\n",
                "\n",
                "df = pd.DataFrame(data)\n",
                "print(f\"Total samples: {len(df)}\")\n",
                "print(f\"Samples with valid predictions: {df['prediction'].notna().sum()}\")\n",
                "print(f\"Samples with missing predictions: {df['prediction'].isna().sum()}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Show samples where prediction extraction failed\n",
                "failed_extractions = df[df['prediction'].isna()]\n",
                "if len(failed_extractions) > 0:\n",
                "    print(f\"\\nFailed extractions ({len(failed_extractions)}):\")\n",
                "    for idx, row in failed_extractions.head(5).iterrows():\n",
                "        print(f\"\\n  Response: {row['response'][:200]}...\")\n",
                "else:\n",
                "    print(\"All predictions extracted successfully!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Filter to valid predictions only\n",
                "df_valid = df[df['prediction'].notna()].copy()\n",
                "df_valid['prediction'] = df_valid['prediction'].astype(int)\n",
                "\n",
                "print(f\"\\nValid predictions: {len(df_valid)} / {len(df)} ({100*len(df_valid)/len(df):.1f}%)\")\n",
                "print(f\"\\nPrediction distribution:\")\n",
                "print(df_valid['prediction'].value_counts())\n",
                "print(f\"\\nGround truth distribution (survival):\")\n",
                "print(df_valid['ground_truth_survival'].value_counts())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Confusion Matrix"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "y_true = df_valid['ground_truth_survival'].values\n",
                "y_pred = df_valid['prediction'].values\n",
                "\n",
                "# Compute confusion matrix\n",
                "cm = confusion_matrix(y_true, y_pred)\n",
                "\n",
                "# Plot confusion matrix\n",
                "fig, ax = plt.subplots(figsize=(8, 6))\n",
                "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Death (0)', 'Survival (1)'])\n",
                "disp.plot(cmap='Blues', ax=ax, values_format='d')\n",
                "plt.title('Confusion Matrix: LLM Survival Predictions vs Ground Truth', fontsize=14)\n",
                "plt.tight_layout()\n",
                "plt.savefig('confusion_matrix_rag.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "# Print raw values\n",
                "print(\"\\nConfusion Matrix:\")\n",
                "print(f\"  TN (True Death):     {cm[0,0]}\")\n",
                "print(f\"  FP (False Survival): {cm[0,1]}\")\n",
                "print(f\"  FN (False Death):    {cm[1,0]}\")\n",
                "print(f\"  TP (True Survival):  {cm[1,1]}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Classification Metrics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calculate metrics\n",
                "accuracy = accuracy_score(y_true, y_pred)\n",
                "precision = precision_score(y_true, y_pred, zero_division=0)\n",
                "recall = recall_score(y_true, y_pred, zero_division=0)\n",
                "f1 = f1_score(y_true, y_pred, zero_division=0)\n",
                "\n",
                "# Specificity (true negative rate)\n",
                "tn, fp, fn, tp = cm.ravel()\n",
                "specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
                "\n",
                "# Print metrics\n",
                "print(\"=\"*50)\n",
                "print(\"CLASSIFICATION METRICS\")\n",
                "print(\"=\"*50)\n",
                "print(f\"Accuracy:    {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
                "print(f\"Precision:   {precision:.4f}\")\n",
                "print(f\"Recall:      {recall:.4f} (Sensitivity)\")\n",
                "print(f\"Specificity: {specificity:.4f}\")\n",
                "print(f\"F1-Score:    {f1:.4f}\")\n",
                "print(\"=\"*50)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Full classification report\n",
                "print(\"\\nDetailed Classification Report:\")\n",
                "print(classification_report(y_true, y_pred, target_names=['Death (0)', 'Survival (1)']))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. ROC Curve and AUROC"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calculate ROC curve (using predictions as scores since they're binary)\n",
                "# For a proper AUROC, you'd want probability scores, but we'll use binary predictions\n",
                "try:\n",
                "    auroc = roc_auc_score(y_true, y_pred)\n",
                "    fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n",
                "    \n",
                "    fig, ax = plt.subplots(figsize=(8, 6))\n",
                "    ax.plot(fpr, tpr, 'b-', linewidth=2, label=f'LLM (AUROC = {auroc:.4f})')\n",
                "    ax.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random (AUROC = 0.5)')\n",
                "    ax.set_xlabel('False Positive Rate', fontsize=12)\n",
                "    ax.set_ylabel('True Positive Rate', fontsize=12)\n",
                "    ax.set_title('ROC Curve: LLM Survival Predictions', fontsize=14)\n",
                "    ax.legend(loc='lower right')\n",
                "    ax.set_xlim([0, 1])\n",
                "    ax.set_ylim([0, 1.05])\n",
                "    plt.tight_layout()\n",
                "    plt.savefig('roc_curve_rag.png', dpi=150, bbox_inches='tight')\n",
                "    plt.show()\n",
                "    \n",
                "    print(f\"\\nAUROC: {auroc:.4f}\")\n",
                "except ValueError as e:\n",
                "    print(f\"Could not compute AUROC: {e}\")\n",
                "    print(\"This may happen if all predictions are the same class.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Precision-Recall Curve and AUPRC"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "try:\n",
                "    # Calculate precision-recall curve\n",
                "    precision_curve, recall_curve, _ = precision_recall_curve(y_true, y_pred)\n",
                "    auprc = average_precision_score(y_true, y_pred)\n",
                "    \n",
                "    # Baseline (proportion of positive class)\n",
                "    baseline = y_true.mean()\n",
                "    \n",
                "    fig, ax = plt.subplots(figsize=(8, 6))\n",
                "    ax.plot(recall_curve, precision_curve, 'b-', linewidth=2, label=f'LLM (AUPRC = {auprc:.4f})')\n",
                "    ax.axhline(y=baseline, color='k', linestyle='--', linewidth=1, label=f'Baseline (AUPRC = {baseline:.4f})')\n",
                "    ax.set_xlabel('Recall', fontsize=12)\n",
                "    ax.set_ylabel('Precision', fontsize=12)\n",
                "    ax.set_title('Precision-Recall Curve: LLM Survival Predictions', fontsize=14)\n",
                "    ax.legend(loc='lower left')\n",
                "    ax.set_xlim([0, 1])\n",
                "    ax.set_ylim([0, 1.05])\n",
                "    plt.tight_layout()\n",
                "    plt.savefig('pr_curve_rag.png', dpi=150, bbox_inches='tight')\n",
                "    plt.show()\n",
                "    \n",
                "    print(f\"\\nAUPRC: {auprc:.4f}\")\n",
                "    print(f\"Baseline (no-skill): {baseline:.4f}\")\n",
                "except ValueError as e:\n",
                "    print(f\"Could not compute AUPRC: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Performance by Cancer System"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calculate metrics per cancer system\n",
                "cancer_systems = df_valid['cancer_system'].unique()\n",
                "system_metrics = []\n",
                "\n",
                "for system in cancer_systems:\n",
                "    subset = df_valid[df_valid['cancer_system'] == system]\n",
                "    if len(subset) < 5:  # Skip systems with too few samples\n",
                "        continue\n",
                "    \n",
                "    y_true_sub = subset['ground_truth_survival'].values\n",
                "    y_pred_sub = subset['prediction'].values\n",
                "    \n",
                "    system_metrics.append({\n",
                "        'Cancer System': system,\n",
                "        'N': len(subset),\n",
                "        'Accuracy': accuracy_score(y_true_sub, y_pred_sub),\n",
                "        'Precision': precision_score(y_true_sub, y_pred_sub, zero_division=0),\n",
                "        'Recall': recall_score(y_true_sub, y_pred_sub, zero_division=0),\n",
                "        'F1': f1_score(y_true_sub, y_pred_sub, zero_division=0),\n",
                "    })\n",
                "\n",
                "df_systems = pd.DataFrame(system_metrics).sort_values('Accuracy', ascending=False)\n",
                "print(\"Performance by Cancer System:\")\n",
                "print(df_systems.to_string(index=False))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize accuracy by cancer system\n",
                "if len(df_systems) > 0:\n",
                "    fig, ax = plt.subplots(figsize=(10, 6))\n",
                "    bars = ax.barh(df_systems['Cancer System'], df_systems['Accuracy'], color='steelblue')\n",
                "    ax.axvline(x=accuracy, color='red', linestyle='--', linewidth=2, label=f'Overall: {accuracy:.2f}')\n",
                "    ax.set_xlabel('Accuracy', fontsize=12)\n",
                "    ax.set_title('Prediction Accuracy by Cancer System', fontsize=14)\n",
                "    ax.set_xlim([0, 1])\n",
                "    ax.legend()\n",
                "    \n",
                "    # Add value labels\n",
                "    for bar, acc in zip(bars, df_systems['Accuracy']):\n",
                "        ax.text(acc + 0.01, bar.get_y() + bar.get_height()/2, f'{acc:.2f}', va='center')\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    plt.savefig('accuracy_by_system_rag.png', dpi=150, bbox_inches='tight')\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Error Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Analyze errors\n",
                "df_valid['correct'] = df_valid['prediction'] == df_valid['ground_truth_survival']\n",
                "df_valid['error_type'] = 'Correct'\n",
                "df_valid.loc[(df_valid['prediction'] == 1) & (df_valid['ground_truth_survival'] == 0), 'error_type'] = 'False Positive (predicted survival, actually died)'\n",
                "df_valid.loc[(df_valid['prediction'] == 0) & (df_valid['ground_truth_survival'] == 1), 'error_type'] = 'False Negative (predicted death, actually survived)'\n",
                "\n",
                "print(\"Error Type Distribution:\")\n",
                "print(df_valid['error_type'].value_counts())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Survival months distribution for correct vs incorrect predictions\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# Correct predictions\n",
                "correct = df_valid[df_valid['correct']]\n",
                "incorrect = df_valid[~df_valid['correct']]\n",
                "\n",
                "axes[0].hist(correct['survival_months'], bins=30, alpha=0.7, color='green', edgecolor='black')\n",
                "axes[0].set_xlabel('Survival Months')\n",
                "axes[0].set_ylabel('Count')\n",
                "axes[0].set_title(f'Correct Predictions (n={len(correct)})')\n",
                "\n",
                "axes[1].hist(incorrect['survival_months'], bins=30, alpha=0.7, color='red', edgecolor='black')\n",
                "axes[1].set_xlabel('Survival Months')\n",
                "axes[1].set_ylabel('Count')\n",
                "axes[1].set_title(f'Incorrect Predictions (n={len(incorrect)})')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('survival_distribution_by_correctness_rag.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "print(f\"\\nMean survival months (correct):   {correct['survival_months'].mean():.1f}\")\n",
                "print(f\"Mean survival months (incorrect): {incorrect['survival_months'].mean():.1f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Summary Dashboard"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create summary\n",
                "print(\"=\" * 60)\n",
                "print(\"          LLM SURVIVAL PREDICTION - SUMMARY REPORT\")\n",
                "print(\"=\" * 60)\n",
                "print(f\"\\nDataset:\")\n",
                "print(f\"  Total samples:           {len(df)}\")\n",
                "print(f\"  Valid predictions:       {len(df_valid)} ({100*len(df_valid)/len(df):.1f}%)\")\n",
                "print(f\"  Ground truth survival:   {y_true.sum()} ({100*y_true.mean():.1f}%)\")\n",
                "print(f\"  Ground truth death:      {len(y_true) - y_true.sum()} ({100*(1-y_true.mean()):.1f}%)\")\n",
                "print(f\"\\nPrimary Metrics:\")\n",
                "print(f\"  Accuracy:                {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
                "print(f\"  F1-Score:                {f1:.4f}\")\n",
                "try:\n",
                "    print(f\"  AUROC:                   {auroc:.4f}\")\n",
                "except:\n",
                "    print(f\"  AUROC:                   N/A\")\n",
                "try:\n",
                "    print(f\"  AUPRC:                   {auprc:.4f}\")\n",
                "except:\n",
                "    print(f\"  AUPRC:                   N/A\")\n",
                "print(f\"\\nDetailed Metrics:\")\n",
                "print(f\"  Precision:               {precision:.4f}\")\n",
                "print(f\"  Recall (Sensitivity):    {recall:.4f}\")\n",
                "print(f\"  Specificity:             {specificity:.4f}\")\n",
                "print(f\"\\nConfusion Matrix:\")\n",
                "print(f\"  True Negatives (TN):     {tn}\")\n",
                "print(f\"  False Positives (FP):    {fp}\")\n",
                "print(f\"  False Negatives (FN):    {fn}\")\n",
                "print(f\"  True Positives (TP):     {tp}\")\n",
                "print(\"=\" * 60)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save metrics to JSON for programmatic access\n",
                "metrics_summary = {\n",
                "    'total_samples': len(df),\n",
                "    'valid_predictions': len(df_valid),\n",
                "    'accuracy': float(accuracy),\n",
                "    'precision': float(precision),\n",
                "    'recall': float(recall),\n",
                "    'specificity': float(specificity),\n",
                "    'f1_score': float(f1),\n",
                "    'confusion_matrix': {\n",
                "        'tn': int(tn),\n",
                "        'fp': int(fp),\n",
                "        'fn': int(fn),\n",
                "        'tp': int(tp)\n",
                "    }\n",
                "}\n",
                "\n",
                "try:\n",
                "    metrics_summary['auroc'] = float(auroc)\n",
                "except:\n",
                "    pass\n",
                "\n",
                "try:\n",
                "    metrics_summary['auprc'] = float(auprc)\n",
                "except:\n",
                "    pass\n",
                "\n",
                "with open('metrics_summary_rag.json', 'w') as f:\n",
                "    json.dump(metrics_summary, f, indent=2)\n",
                "\n",
                "print(\"Metrics saved to metrics_summary_rag.json\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.19"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
